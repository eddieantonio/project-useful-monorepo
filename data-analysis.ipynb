{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: to get this working, you must install the `data-analysis` group dependencies using Poetry:\n",
    "\n",
    "```shell\n",
    "poetry install --with=data-analysis\n",
    "```\n",
    "\n",
    "You should also run this if you are using git:\n",
    "\n",
    "```shell\n",
    "poetry run nbstripout --install\n",
    "```\n",
    "\n",
    "This will automatically clear output when committing to git :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import sqlite3\n",
    "from operator import itemgetter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from data_analysis_utils import agreement_as_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to index and group data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by each Java source code file (context or unit):\n",
    "BY_CONTEXT = [\"srcml_path\", \"version\"]\n",
    "# by the PEM variant shown to the rater:\n",
    "BY_SCENARIO = BY_CONTEXT + [\"variant\"]\n",
    "# by the rater: this uniquely identifies one particular data point:\n",
    "BY_RATER = BY_SCENARIO + [\"rater\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might as well load all of the scenarios, indexed by `srcml_path` and `version`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample.pickle\", \"rb\") as sample_file:\n",
    "    _ALL_SCENARIOS = pickle.load(sample_file)\n",
    "\n",
    "ALL_SCENARIOS = {\n",
    "    (scenario[\"xml_filename\"], scenario[\"version\"]): scenario\n",
    "    for scenario in _ALL_SCENARIOS\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also load the Decaf PEMs, because it will be necessary later to manually compare Decaf's output to the javac PEM under question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"decaf.pickle\", \"rb\") as decaf_file:\n",
    "    DECAF_RESPONSES = pickle.load(decaf_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a basic dataframe from `answers.sqlite3`. However, the `answers` column will need to be parsed as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read answers.sqlite3 into a DataFrame\n",
    "conn = sqlite3.connect(\"answers.sqlite3\")\n",
    "df = pd.read_sql_query(\"SELECT * FROM answers\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the data from the JSON column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to clean the data — extract it from that JSON column.\n",
    "\n",
    "Instead of creating a nice schema in `answers.sqlite3`, I decided to defer the job of making nice columns to the data analysis stage. So now we have to parse the answers column as JSON and extract data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As of 2023-05-02, these are columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(df[\"answers\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_columns(df):\n",
    "    json_column = df[\"answers\"].apply(json.loads)\n",
    "    return df.assign(\n",
    "        jargon=(\n",
    "            json_column.apply(itemgetter(\"jargon\")).astype(int)\n",
    "        ),\n",
    "        sentence_structure=(\n",
    "            json_column.apply(itemgetter(\"sentence_structure\")).astype(\"category\")\n",
    "        ),\n",
    "        # Explanation should already be a boolean column, that is completely filled in:\n",
    "        explanation=(\n",
    "            json_column.apply(itemgetter(\"explanation\"))\n",
    "        ),\n",
    "        explanation_correctness=(\n",
    "            json_column.apply(itemgetter(\"explanation_correctness\")).astype(\"category\")\n",
    "        ),\n",
    "        # I wish I had a better name for this column, but it's basically, \"if the explanation is MAYBE correct, WHY is it maybe correct?\"\n",
    "        explanation_maybe=(\n",
    "            json_column.apply(itemgetter(\"explanation_maybe\")).astype(\"category\")\n",
    "        ),\n",
    "        fix=(\n",
    "            json_column.apply(itemgetter(\"fix\")).fillna(\"N/A\").astype(\"category\")\n",
    "        ),\n",
    "        fix_correctness=(\n",
    "            json_column.apply(itemgetter(\"fix_correctness\")).astype(\"category\")\n",
    "        ),\n",
    "        additional_errors=(\n",
    "            json_column.apply(itemgetter(\"additional_errors\")).astype(\"category\")\n",
    "        ),\n",
    "        notes=(\n",
    "            json_column.apply(itemgetter(\"notes\")).astype(\"string\")\n",
    "        ),\n",
    "        length=(\n",
    "            json_column.apply(itemgetter(\"length\")).astype(int)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def variant_as_categorical(df):\n",
    "    variant = df[\"variant\"].astype(\"category\")\n",
    "    assert len(variant.cat.categories) == 4\n",
    "    return df\n",
    "\n",
    "\n",
    "def rater_as_categorical(df):\n",
    "    rater = df[\"rater\"].astype(\"category\")\n",
    "    assert len(rater.cat.categories) == 3\n",
    "    return df.assign(rater=rater)\n",
    "\n",
    "\n",
    "def set_empty_notes_to_na(df):\n",
    "    \"Notes that are empty strings should just be missing values\"\n",
    "    return df.assign(notes=df[\"notes\"].replace(\"\", pd.NA))\n",
    "\n",
    "expanded_df = df\\\n",
    "    .pipe(json_to_columns)\\\n",
    "    .drop(columns=[\"answers\"])\\\n",
    "    .pipe(variant_as_categorical)\\\n",
    "    .pipe(rater_as_categorical)\\\n",
    "    .pipe(set_empty_notes_to_na)\n",
    "\n",
    "expanded_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the levels of the categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the levels for each categorical variable in df\n",
    "def get_levels(df, skip=()):\n",
    "    for column in df.select_dtypes(\"category\").columns:\n",
    "        if column in skip:\n",
    "            continue\n",
    "        print(df[column].value_counts())\n",
    "        print(set(df[column].cat.categories))\n",
    "        print()\n",
    "\n",
    "\n",
    "get_levels(expanded_df, skip=[\"variant\", \"rater\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating `full_df`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further data analysis, it's useful to think of some of the categorical responses as being ordinal. It will also smooth over inter-rater reliability (you could argue that it's cooking the books), since if two raters answer \"yes\" and \"maybe\", that's more agreement than two raters saying \"yes\" and \"no\".\n",
    "\n",
    "There are also some columns that can be made binary, like \"does it provide a correct fix?\" or \"is the explanation definitely correct?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMAPPINGS = dict(\n",
    "    sentence_structure_ordinal={\"unclear\": 0, \"could-be-clearer\": 1, \"clear\": 2},\n",
    "    explanation_correctness_ordinal={\"no\": 0, \"maybe\": 1, \"yes\": 2},\n",
    "    fix_ordinal={\n",
    "        \"no\": 0,\n",
    "        \"implicit-suggestion\": 1,\n",
    "        \"generic\": 2,\n",
    "        \"hint\": 3,\n",
    "        \"confident\": 4,\n",
    "    },\n",
    "    fix_correctness_ordinal={\"no\": 0, \"maybe\": 1, \"yes\": 2},\n",
    "    additional_errors_ordinal={\"no\": 0, \"maybe\": 1, \"yes\": 2},\n",
    "    # Is the sentence structure clear?\n",
    "    sentence_structure_binary={\"unclear\": 0, \"could-be-clearer\": 0, \"clear\": 1},\n",
    "    # Can the explanation be reasonably considered to be correct?\n",
    "    explanation_correctness_binary={\"no\": 0, \"maybe\": 1, \"yes\": 1},\n",
    "    # Is **any** kind of fix suggested?\n",
    "    fix_binary_levels={\n",
    "        \"no\": 0,\n",
    "        \"implicit-suggestion\": 1,\n",
    "        \"generic\": 1,\n",
    "        \"hint\": 1,\n",
    "        \"confident\": 1,\n",
    "    },\n",
    "    # Is the fix DEFINITELY correct?\n",
    "    fix_correctness_binary={\"no\": 0, \"maybe\": 0, \"yes\": 1},\n",
    "    additional_errors_binary={\"no\": 0, \"maybe\": 1, \"yes\": 1},\n",
    ")\n",
    "\n",
    "def with_correct_dtype(df, column_name):\n",
    "    return df[column_name].map(REMAPPINGS[column_name])\n",
    "\n",
    "\n",
    "def add_ordinal_columns(df):\n",
    "    return df.assign(\n",
    "        sentence_structure_ordinal=df[\"sentence_structure\"].map(REMAPPINGS[\"sentence_structure_ordinal\"]),\n",
    "        explanation_correctness_ordinal=df[\"explanation_correctness\"].map(REMAPPINGS[\"explanation_correctness_ordinal\"]),\n",
    "        fix_ordinal=df[\"fix\"].map(REMAPPINGS[\"fix_ordinal\"]),\n",
    "        fix_correctness_ordinal=df[\"fix_correctness\"].map(REMAPPINGS[\"fix_correctness_ordinal\"]),\n",
    "        additional_errors_ordinal=df[\"additional_errors\"].map(REMAPPINGS[\"additional_errors_ordinal\"]),\n",
    "    )\n",
    "\n",
    "\n",
    "def add_binary_columns(df):\n",
    "    return df.assign(\n",
    "        sentence_structure_binary=df[\"sentence_structure\"].map(REMAPPINGS[\"sentence_structure_binary\"]),\n",
    "        explanation_correctness_binary=df[\"explanation_correctness\"].map(REMAPPINGS[\"explanation_correctness_binary\"]),\n",
    "        fix_binary=df[\"fix\"].map(REMAPPINGS[\"fix_binary_levels\"]),\n",
    "        fix_correctness_binary=df[\"fix_correctness\"].map(REMAPPINGS[\"fix_correctness_binary\"]),\n",
    "        additional_errors_binary=df[\"additional_errors\"].map(REMAPPINGS[\"additional_errors_binary\"]),\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The perfect fix\n",
    "\n",
    "A perfect rating is when an error message:\n",
    " 1. has an explanation\n",
    " 2. ...that is correct\n",
    " 3. with a confident fix\n",
    " 3. ...that is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_perfect_fix(df):\n",
    "    return df.assign(\n",
    "        perfect_fix=(\n",
    "            # df[\"explanation_correctness\"] being not null occurs if and only if df[\"explanation\"] == True\n",
    "            (df[\"explanation_correctness\"] == \"yes\") &\n",
    "            (df[\"fix\"] == \"confident\") &\n",
    "            (df[\"fix_correctness\"] == \"yes\")\n",
    "        ).astype(bool)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = expanded_df\\\n",
    "    .pipe(add_ordinal_columns)\\\n",
    "    .pipe(add_binary_columns)\\\n",
    "    .pipe(add_perfect_fix)\n",
    "\n",
    "full_df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every rating **must** state whether an explanation was provided or not. Assert this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert full_df[\"explanation\"].isna().sum() == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `lengths`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length should be the equal regardless of the rater, so let's assert that here and get a table we can join with later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_lengths(df):\n",
    "    \"Ensure that each rater has seen a message with the same length.\"\n",
    "    assert (df[\"length\"][\"min\"] == df[\"length\"][\"max\"]).all()\n",
    "    return df\n",
    "\n",
    "\n",
    "def as_sql_style_table(df):\n",
    "    return pd.DataFrame({\n",
    "        \"srcml_path\": df[\"srcml_path\"],\n",
    "        \"version\": df[\"version\"],\n",
    "        \"variant\": df[\"variant\"],\n",
    "        \"length\": df[\"length\"][\"min\"],\n",
    "    })\n",
    "\n",
    "\n",
    "# A common pitfall of using a categorical variable with groupby\n",
    "# is that it does \"some sort of cartesian product\" with the other columns,\n",
    "# which is not what we want.\n",
    "# So we have to set `observed=False` to avoid this.\n",
    "# \n",
    "# See: https://stackoverflow.com/a/67645084\n",
    "message_lengths = full_df\\\n",
    "    .groupby(BY_SCENARIO, as_index=False, observed=False)\\\n",
    "    .agg({\"length\": [\"min\", \"max\"]})\\\n",
    "    .pipe(assert_lengths)\\\n",
    "    .pipe(as_sql_style_table)\n",
    "\n",
    "message_lengths.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few rows we should remove before doing further analysis. These rows come from **Decaf**. Unfortunately, some of these Decaf rows correspond to a _different_ Java error than the one under study, so it would be unfair/invalid to include Decaf in these analyses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and removing ratings that should be skipped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I was rating, I marked a few Decaf PEMs as skippable, because Decaf did not match the same error message as the other variants. In other words, **Decaf was explaining the wrong error**. It's unfair to include these PEMs in the analysis, so let's find them and skip them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_marked_as_skip(df):\n",
    "    return df[df[\"notes\"] == \"SKIP\"]\n",
    "\n",
    "def ensure_variant_is_decaf(df):\n",
    "    assert (df[\"variant\"] == \"decaf\").all()\n",
    "    return df\n",
    "\n",
    "def to_scenario_keys(df):\n",
    "    \"\"\"\n",
    "    Converts the data frame into a set of keys that can be used \n",
    "    to look up the code context in ALL_SCENARIOS.\n",
    "    \"\"\"\n",
    "    return set(df[BY_CONTEXT].itertuples(index=False, name=None))\n",
    "\n",
    "\n",
    "should_skip = full_df\\\n",
    "    .pipe(select_marked_as_skip)\\\n",
    "    .pipe(ensure_variant_is_decaf)\\\n",
    "    .pipe(to_scenario_keys)\n",
    "\n",
    "should_skip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prajish found a few more Decaf PEMs that should be skipped.\n",
    "\n",
    "I... also might not have been diligent when writing SKIP in the column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspected_filenames = {\"Quiz.java\", \"getLargestPerimeterMultipleFiles.java\", \"Game.java\", \"PerimeterAssignmentRunner.java\"}\n",
    "\n",
    "suspected_contexts = []\n",
    "for (srcml_path, version), scenario in ALL_SCENARIOS.items():\n",
    "    unit = scenario[\"unit\"]\n",
    "    if unit.filename in suspected_filenames:\n",
    "        suspected_contexts.append((srcml_path, version))\n",
    "\n",
    "suspected_contexts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's manually inspect these. I'm looking for errors that will be caugth AFTER a parser error, such as \"cannot find symbol\" or \"package does not exist\". If it's a parser error (such as \"`;` expected\"), then Decaf will report the correct error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario_id in suspected_contexts:\n",
    "    scenario = ALL_SCENARIOS[scenario_id]\n",
    "    decaf_output = DECAF_RESPONSES[scenario_id]\n",
    "    print(scenario_id)\n",
    "    print(\"  \", scenario[\"unit\"].pems)\n",
    "    print(\"  \", decaf_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manually reviewing the above errors, here are the ones that I think are definitely skipable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_skip = should_skip.union({\n",
    "    ('/data/mini/srcml-2016-03/project-6635685/src-30958703.xml', '1159101311'),\n",
    "    ('/data/mini/srcml-2018-06/project-12853713/src-61909914.xml', '2493733098'),\n",
    "    ('/data/mini/srcml-2020-06/project-20162892/src-99474416.xml', '4532849754'),\n",
    "    ('/data/mini/srcml-2019-09/project-17095277/src-83478876.xml', '3614846622'),\n",
    "})\n",
    "\n",
    "should_skip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `cleaned_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_decaf_pems(df):\n",
    "    new_df = df\\\n",
    "        .set_index(BY_SCENARIO)\\\n",
    "        .drop(index=[(srcml_path, version, \"decaf\") for srcml_path, version in should_skip])\\\n",
    "        .reset_index()\n",
    "    assert len(should_skip) > 0\n",
    "    assert len(new_df) < len(df)\n",
    "    return new_df\n",
    "\n",
    "cleaned_df = full_df\\\n",
    "    .pipe(remove_decaf_pems)\n",
    "\n",
    "cleaned_df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there should be a few fewer Decaf ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.value_counts(\"variant\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining questions answered by at least 2 raters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assert that all scenarios were answered by at least two raters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_ratings = cleaned_df.groupby([\"srcml_path\", \"version\"]).filter(lambda x: len(x[\"rater\"].unique()) >= 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find messages that have only been rated by Brett. I can assign all of these to myself, and then we have a bit more data to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_columns(df):\n",
    "    return df[[\"pem_category\", \"srcml_path\", \"version\"]]\n",
    "\n",
    "def maybe_save_assignments(df):\n",
    "    \"\"\"\n",
    "    Save new assignments to a file, if there are any scenarios that need an extra rater.\n",
    "    That extra rater will be me, Eddie.\n",
    "    \"\"\"\n",
    "    if len(df) > 0:\n",
    "        df.to_csv(\"additional-eddie-assignments.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "    return df\n",
    "\n",
    "cleaned_df.groupby([\"srcml_path\", \"version\"])\\\n",
    "    .filter(lambda x: len(x[\"rater\"].unique()) == 1)\\\n",
    "    .query('rater == \"brett\"')[['srcml_path', 'version']]\\\n",
    "    .drop_duplicates()\\\n",
    "    .assign(pem_category=\"unknown\")\\\n",
    "    .pipe(rearrange_columns)\\\n",
    "    .pipe(maybe_save_assignments)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of 2023-05-09, I assigned the remaining Brett messages to myself. The only contexts left with only one rater were assigned to me (Eddie) 🙃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(two_ratings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating `ratings`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the ratings of all of raters into one, big table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_COLUMNS = {'srcml_path', 'version', 'variant', 'rater', 'length'}\n",
    "\n",
    "RESPONSE_COLUMNS = [\n",
    "    column for column in full_df.columns\n",
    "    if column not in FIXED_COLUMNS\n",
    "]\n",
    "RESPONSE_COLUMNS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the `assigned` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_assigned(df):\n",
    "    \"\"\"\n",
    "    Add convenience columns that indicates whether a rater was assigned to this scenario.\n",
    "    This only makes sense AFTER pivoting the DataFrame.\n",
    "    \"\"\"\n",
    "    # A rater has given True or False if and only if they were assigned to this scenario.\n",
    "    # Therefore, we can use the presence of NaNs to determine whether a rater was assigned:\n",
    "    assigned = df[\"explanation\"].notna()\n",
    "    # However... we need to reconstruct the hierarchical index before we concatentate it.\n",
    "    index = pd.MultiIndex.from_product([[\"assigned\"], assigned.columns])\n",
    "    assigned_df = pd.DataFrame(assigned.values, columns=index, index=assigned.index)\n",
    "    # I tried to do this using DataFrame.assign, but this doesn't work for hierarchical indexes.\n",
    "    return pd.concat([df, assigned_df], axis=1)\n",
    "\n",
    "# Put all the raters side-by-side using a hierarchical index\n",
    "ratings = two_ratings\\\n",
    "    .drop(columns=[\"length\"])\\\n",
    "    .pivot(index=BY_SCENARIO, columns=\"rater\", values=RESPONSE_COLUMNS)\\\n",
    "    .pipe(add_assigned)\n",
    "\n",
    "ratings.head(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's nice to see this data in an Excel spreadsheet, so OPTIONALLY export it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals().get(\"EXPORT_EXCEL\", False):\n",
    "    ratings.to_excel(\"hierarchical_responses.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-rater agreement between individual raters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: perhaps Scott's π is a better measure of agreement for this data:\n",
    "\n",
    "See: https://stats.stackexchange.com/a/525640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_two_raters(rater1, rater2):\n",
    "    \"Returns a DataFrame with only the rows where both raters have rated the scenario\"\n",
    "    return ratings[ratings[\"assigned\"][rater1] & ratings[\"assigned\"][rater2]]\n",
    "\n",
    "def kappa_for_two_raters(column, rater1, rater2):\n",
    "    \"Returns agreement for two raters on a certain column\"\n",
    "    agreement = group_two_raters(rater1, rater2)\n",
    "    criteria = agreement[column].astype(\"category\")\n",
    "    return cohen_kappa_score(criteria[rater1], criteria[rater2])\n",
    "\n",
    "def kappa_for_two_raters_with_label(column, rater1, rater2):\n",
    "    kappa = kappa_for_two_raters(column, rater1, rater2)\n",
    "    return kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"eddie\", \"prajish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"eddie\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"prajish\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"eddie\", \"prajish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"eddie\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"prajish\", \"brett\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating two synthesized raters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesise two raters by null-coalescing our three raters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_two_raters(df, column_name):\n",
    "    \"\"\"\n",
    "    Synthesize two raters, such that each question has at least one unique rater.\n",
    "    \n",
    "    This uses the following truth table to select the rater:\n",
    "\n",
    "           Assigned?       |        Rater\n",
    "     Eddie  Prajish  Brett |     1         2\n",
    "    -----------------------+---------------------\n",
    "      ❌      ✅       ✅   |  Brett      Prajish\n",
    "      ✅      ❌       ✅   |  Eddie      Brett\n",
    "      ✅      ✅       ❌   |  Eddie      Prajish\n",
    "    -----------------------+---------------------\n",
    "      ✅      ✅       ✅   |  Eddie      Prajish\n",
    "\n",
    "    \"\"\"\n",
    "    rater1 = df[column_name][\"eddie\"].where(df[\"assigned\"][\"eddie\"], df[column_name][\"brett\"])\n",
    "    rater2 = df[column_name][\"prajish\"].where(df[\"assigned\"][\"prajish\"], df[column_name][\"brett\"])\n",
    "\n",
    "    return rater1, rater2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_NULL_COLUMNS = [\n",
    "    \"jargon\",\n",
    "    \"sentence_structure\",\n",
    "    \"explanation\",\n",
    "    \"fix\",\n",
    "    \"additional_errors\",\n",
    "]\n",
    "\n",
    "\n",
    "def to_correct_dtype(column, column_name):\n",
    "    type_ = column_name.split(\"_\")[-1]\n",
    "    \n",
    "    n_nulls = column.isnull().sum()\n",
    "    if n_nulls > 0:\n",
    "        assert column_name not in NON_NULL_COLUMNS, f\"Column '{column_name}' has missing values\"\n",
    "        # Do not convert columns with nulls.\n",
    "        return column\n",
    "\n",
    "    if column_name == \"perfect_fix\" or type_ == \"binary\":\n",
    "        return column.astype(bool)\n",
    "    elif column_name == \"jargon\" or type_ == \"ordinal\":\n",
    "        return column.astype(int)\n",
    "    else:\n",
    "        return column.astype(\"category\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the `two_raters` data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_for_two_raters(df, column_name):\n",
    "    rater1, rater2 = create_two_raters(df, column_name)\n",
    "    index = pd.MultiIndex.from_product([[column_name], [\"rater1\", \"rater2\"]], names=[None, \"rater\"])\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            (column_name, \"rater1\"): to_correct_dtype(rater1, column_name),\n",
    "            (column_name, \"rater2\"): to_correct_dtype(rater2, column_name),\n",
    "        },\n",
    "        columns=index, index=df.index\n",
    "    )\n",
    "\n",
    "two_raters = pd.concat([column_for_two_raters(ratings, column) for column in RESPONSE_COLUMNS], axis=1)\n",
    "\n",
    "if globals().get(\"EXPORT_EXCEL\", False):\n",
    "    two_raters.to_excel(\"two_raters.xlsx\")\n",
    "\n",
    "two_raters.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_rater_agreement(column_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Return the inter-rater reliability for a certain column for the two synthesized raters.\n",
    "    Note: the column must not have any missing values.\n",
    "    \"\"\"\n",
    "    criteria = two_raters[column_name]\n",
    "    rater1 = criteria[\"rater1\"]\n",
    "    rater2 = criteria[\"rater2\"]\n",
    "    \n",
    "    assert rater1.isnull().sum() == 0\n",
    "    assert rater2.isnull().sum() == 0\n",
    "\n",
    "    kappa = cohen_kappa_score(rater1, rater2, **kwargs)\n",
    "    return kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-rater inter-rater reliability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the agreement for non-null categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rater_agreement(\"sentence_structure\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ordinal data, apply weighting to smooth the agreement.\n",
    "\n",
    "See here for weighting: https://github.com/jmgirard/mReliability/wiki/Weighting-scheme\n",
    "\n",
    "I think linear weighting is fair. Quadratic is waaaaay too forgiving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rater_agreement(\"sentence_structure_ordinal\", weights=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rater_agreement(\"explanation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rater_agreement(\"fix\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to report how we decided to interpret the fix categories as ordinal data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rater_agreement(\"fix_ordinal\", weights=\"linear\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if Decaf data affects this: Brett and I interpretted this differently\n",
    "\n",
    "\"Researchers agreed that if Decaf found errors but did not discuss them -- turn into count\"\n",
    "\n",
    "Report count. Might not even report this for multiple errors.\n",
    "\n",
    "IRR doesn't make much sense here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rater_agreement(\"additional_errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rater_agreement(\"additional_errors_ordinal\", weights=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rater_agreement(\"perfect_fix\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: How to measure the agreement of columns with nulls?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: descriptive statistics of jargon\n",
    "\n",
    "**TODO**: plots of message length\n",
    "\n",
    "**TODO**: number of additional errors reported...?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many scenarios have been rated by each rater?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.groupby(\"rater\").size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many variants were rated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_raters.groupby(\"variant\").size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decaf is missing 3 ratings because of the messages excluded above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack the data to obtain a long-format table\n",
    "\n",
    "The `two_raters` data frame has a hierarchical _column_ index, but most analyses want a long-format table. To obtain this, we can stack (pivot the columns) to a column of its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_raters_long = two_raters.stack(\"rater\")\n",
    "\n",
    "two_raters_long.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_lengths.groupby(\"variant\").agg({\"length\": [\"min\", \"max\", \"mean\", \"std\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=message_lengths,\n",
    "            x=\"variant\",\n",
    "            y=\"length\",\n",
    "            order=[\"javac\", \"decaf\", \"gpt-4-error-only\", \"gpt-4-with-context\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jargon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_raters_long.groupby(\"variant\").agg({\"jargon\": [\"min\", \"max\", \"mean\", \"std\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=cleaned_df,\n",
    "                x=\"jargon\",\n",
    "                hue=\"variant\",\n",
    "                discrete=True,\n",
    "                multiple=\"dodge\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportions of ratings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often do the error messages provide explanations of the error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_raters_long.groupby(\"variant\")[\"explanation\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_raters_long.groupby([\"variant\", \"rater\"])[\"explanation\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_raters_long.groupby([\"variant\", \"rater\"])[\"explanation_correctness\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_raters_long.groupby(\"variant\")[\"fix\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_raters_long.groupby([\"variant\", \"rater\"])[\"fix\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_raters_long.groupby(\"variant\")[\"fix_correctness\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_raters_long.groupby([\"variant\", \"rater\"])[\"fix_correctness\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_raters_long\\\n",
    "    .groupby(\"variant\")[\"perfect_fix\"]\\\n",
    "    .value_counts(normalize=True)\\\n",
    "    .sort_index() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_raters_long\\\n",
    "    .groupby([\"variant\", \"rater\"])[\"perfect_fix\"]\\\n",
    "    .value_counts(normalize=True)\\\n",
    "    .sort_index() * 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: IRR of _perfect fix_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SCENARIOS[('/data/mini/srcml-2019-06/project-16564363/src-80577273.xml', '3470043524')][\"unit\"].pems[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-useful-monorepo-alBd81bV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
