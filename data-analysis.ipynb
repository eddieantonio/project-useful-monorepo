{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: to get this working, you must install the `data-analysis` group dependencies using Poetry:\n",
    "\n",
    "```shell\n",
    "poetry install --with=data-analysis\n",
    "```\n",
    "\n",
    "You should also run this if you are using git:\n",
    "\n",
    "```shell\n",
    "poetry run nbstripout --install\n",
    "```\n",
    "\n",
    "This will automatically clear output when committing to git :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a basic dataframe from `answers.sqlite3`. Then we'll tease out the JSON columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from data_analysis_utils import agreement_as_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to index and group data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by each Java source code file (unit):\n",
    "BY_UNIT = [\"srcml_path\", \"version\"]\n",
    "# by the PEM variant shown to the rater:\n",
    "BY_SCENARIO = BY_UNIT + [\"variant\"]\n",
    "# by the rater: uniquely identifies one particular data point:\n",
    "BY_RATER = BY_SCENARIO + [\"rater\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read answers.sqlite3 into a DataFrame\n",
    "conn = sqlite3.connect(\"answers.sqlite3\")\n",
    "df = pd.read_sql_query(\"SELECT * FROM answers\", conn)\n",
    "conn.close()\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to clean the data â€” extract it from that JSON column.\n",
    "\n",
    "Instead of creating a nice schema in `answers.sqlite3`, I decided to defer the job of making nice columns to the data analysis stage. So now we have to parse the answers column as JSON and extract data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As of 2023-05-02, these are columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"answers\"].apply(json.loads)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_columns(df):\n",
    "    json_column = df[\"answers\"].apply(json.loads)\n",
    "    return df.assign(\n",
    "        jargon=json_column.apply(lambda x: x[\"jargon\"]).astype(int),\n",
    "        sentence_structure=json_column.apply(lambda x: x[\"sentence_structure\"]).astype(\"category\"),\n",
    "        explanation=json_column.apply(lambda x: x[\"explanation\"]),\n",
    "        explanation_correctness=json_column.apply(lambda x: x[\"explanation_correctness\"]).astype(\"category\"),\n",
    "        # I wish I had a better name for this column, but it's basically, \"if the explanation is MAYBE correct, WHY is it maybe correct?\"\n",
    "        explanation_maybe=json_column.apply(lambda x: x[\"explanation_maybe\"]).astype(\"category\"),\n",
    "        fix=json_column.apply(lambda x: x[\"fix\"]).astype(\"category\"),\n",
    "        fix_correctness=json_column.apply(lambda x: x[\"fix_correctness\"]).astype(\"category\"),\n",
    "        additional_errors=json_column.apply(lambda x: x[\"additional_errors\"]).astype(\"category\"),\n",
    "        notes=json_column.apply(lambda x: x[\"notes\"]).astype(\"string\"),\n",
    "        length=json_column.apply(lambda x: x[\"length\"]).astype(int),\n",
    "    )\n",
    "\n",
    "def variant_and_rater_as_categorical(df):\n",
    "    \"variant and rater are both fixed, categorical variables\"\n",
    "    return df.assign(\n",
    "        variant=df[\"variant\"].astype(\"category\"),\n",
    "        rater=df[\"rater\"].astype(\"category\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def set_empty_notes_to_na(df):\n",
    "    \"Notes that are empty strings should just be missing values\"\n",
    "    return df.assign(notes=df[\"notes\"].replace(\"\", pd.NA))\n",
    "\n",
    "\n",
    "full_df = df.pipe(variant_and_rater_as_categorical)\\\n",
    "    .pipe(json_to_columns)\\\n",
    "    .pipe(set_empty_notes_to_na)\\\n",
    "    .drop(columns=[\"answers\"])\n",
    "\n",
    "\n",
    "full_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[\"rater\"].nunique() == 3\n",
    "assert df[\"variant\"].nunique() == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHY IS THIS 900??!?!?\n",
    "full_df.groupby([\"srcml_path\", \"version\", \"variant\"]).size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length should be the equal regardless of the rater, so let's assert that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lengths(df):\n",
    "    lengths = df.groupby(BY_SCENARIO).agg({\"length\": [\"min\", \"max\"]})\n",
    "    assert (lengths[\"length\"][\"min\"] == lengths[\"length\"][\"max\"]).all()\n",
    "\n",
    "# idk why this doesn't work any more \n",
    "#check_lengths(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_ratings = full_df.groupby([\"srcml_path\", \"version\"]).filter(lambda x: len(x[\"rater\"].unique()) >= 2)\n",
    "assert len(two_ratings) == len(full_df), \"Had fewer than two ratings for some scenarios\"\n",
    "\n",
    "two_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_RENAME = [\n",
    "    \"jargon\",\n",
    "    \"sentence_structure\",\n",
    "    \"explanation\",\n",
    "    \"explanation_correctness\",\n",
    "    \"explanation_maybe\",\n",
    "    \"fix\",\n",
    "    \"fix_correctness\",\n",
    "    \"additional_errors\",\n",
    "    \"notes\",\n",
    "    \"length\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_ratings(df, rater):\n",
    "    return (\n",
    "        df[df[\"rater\"] == rater]\n",
    "        .set_index(BY_SCENARIO)\n",
    "        # We don't need the rater column anymore\n",
    "        # and earlier we confirmed that the length column is identical\n",
    "        .drop(columns=[\"rater\", \"length\"])\n",
    "        # We have to rename the columns here because the three-way merge will fail to recognize identical columns\n",
    "        # otherwise:\n",
    "        .rename(columns={col: f\"{col}_{rater}\" for col in COLUMNS_TO_RENAME})\n",
    "    )\n",
    "\n",
    "# Get the explanation ratings from prajish and eddie:\n",
    "eddie_ratings = get_ratings(two_ratings, \"eddie\")\n",
    "prajish_ratings = get_ratings(two_ratings, \"prajish\")\n",
    "brett_ratings = get_ratings(two_ratings, \"brett\")\n",
    "\n",
    "assert len(eddie_ratings) == len(prajish_ratings) == len(brett_ratings)\n",
    "assert len(eddie_ratings) == 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = prajish_ratings.join([eddie_ratings, brett_ratings], how=\"outer\")\n",
    "\n",
    "ratings.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddie_and_prajish_agreement = ratings[ratings[\"explanation_eddie\"].notna() & ratings[\"explanation_prajish\"].notna()]\n",
    "eddie_and_prajish_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps Scott's pi is a better measure of agreement for this data:\n",
    "# See: https://stats.stackexchange.com/a/525640\n",
    "\n",
    "eddie_and_prajish_agreement = ratings[ratings[\"explanation_eddie\"].notna() & ratings[\"explanation_prajish\"].notna()]\n",
    "\n",
    "kappa = cohen_kappa_score(eddie_and_prajish_agreement[\"explanation_eddie\"].astype(bool), eddie_and_prajish_agreement[\"explanation_prajish\"].astype(bool))\n",
    "kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_two_raters(rater1, rater2):\n",
    "    \"Returns a DataFrame with only the rows where both raters have rated the scenario\"\n",
    "    return ratings[ratings[f\"explanation_{rater1}\"].notna() & ratings[f\"explanation_{rater2}\"].notna()]\n",
    "\n",
    "def kappa_for_two_raters(column, rater1, rater2):\n",
    "    \"Returns agreement for two raters on a certain column\"\n",
    "    agreement = group_two_raters(rater1, rater2)\n",
    "    return cohen_kappa_score(agreement[f\"{column}_{rater1}\"].astype(\"category\"), agreement[f\"{column}_{rater2}\"].astype(\"category\"))\n",
    "\n",
    "def kappa_for_two_raters_with_label(column, rater1, rater2):\n",
    "    kappa = kappa_for_two_raters(column, rater1, rater2)\n",
    "    return kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"eddie\", \"prajish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"eddie\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"prajish\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"eddie\", \"prajish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"eddie\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"prajish\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddie = ratings[\"explanation_eddie\"]\n",
    "brett = ratings[\"explanation_brett\"]\n",
    "prajish = ratings[\"explanation_prajish\"]\n",
    "\n",
    "rater1 = prajish.combine_first(brett)\n",
    "rater2 = eddie.combine_first(brett)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = cohen_kappa_score(rater1.astype(\"category\"), rater2.astype(\"category\"))\n",
    "kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a summary that is (somewhat) easy to display in Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df\\\n",
    "    .assign(explanation=full_df[\"explanation\"].astype(int))\\\n",
    "    .pivot(index=[\"srcml_path\", \"version\", \"variant\"], columns=\"rater\", values=[\"explanation\", \"fix\", \"notes\"]).to_excel(\"full_df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-useful-monorepo-alBd81bV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
