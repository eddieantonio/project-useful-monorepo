{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: to get this working, you must install the `data-analysis` group dependencies using Poetry:\n",
    "\n",
    "```shell\n",
    "poetry install --with=data-analysis\n",
    "```\n",
    "\n",
    "You should also run this if you are using git:\n",
    "\n",
    "```shell\n",
    "poetry run nbstripout --install\n",
    "```\n",
    "\n",
    "This will automatically clear output when committing to git :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from data_analysis_utils import agreement_as_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to index and group data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by each Java source code file (unit):\n",
    "# TODO: rename to \"context\"?\n",
    "BY_UNIT = [\"srcml_path\", \"version\"]\n",
    "# by the PEM variant shown to the rater:\n",
    "BY_SCENARIO = BY_UNIT + [\"variant\"]\n",
    "# by the rater: this uniquely identifies one particular data point:\n",
    "BY_RATER = BY_SCENARIO + [\"rater\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a basic dataframe from `answers.sqlite3`. However, the `answers` column will need to be parsed as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read answers.sqlite3 into a DataFrame\n",
    "conn = sqlite3.connect(\"answers.sqlite3\")\n",
    "df = pd.read_sql_query(\"SELECT * FROM answers\", conn)\n",
    "conn.close()\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to clean the data â€” extract it from that JSON column.\n",
    "\n",
    "Instead of creating a nice schema in `answers.sqlite3`, I decided to defer the job of making nice columns to the data analysis stage. So now we have to parse the answers column as JSON and extract data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As of 2023-05-02, these are columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(df[\"answers\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"answers\"].apply(json.loads).apply(itemgetter(\"fix\")).value_counts()\n",
    "df[\"answers\"].apply(json.loads).apply(lambda x: x[\"fix\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_columns(df):\n",
    "    json_column = df[\"answers\"].apply(json.loads)\n",
    "    return df.assign(\n",
    "        jargon=(\n",
    "            json_column.apply(itemgetter(\"jargon\")).astype(int)\n",
    "        ),\n",
    "        sentence_structure=(\n",
    "            json_column.apply(itemgetter(\"sentence_structure\")).astype(\"category\")\n",
    "        ),\n",
    "        # Explanation should already be a boolean column, that is completely filled in:\n",
    "        explanation=(\n",
    "            json_column.apply(itemgetter(\"explanation\"))\n",
    "        ),\n",
    "        explanation_correctness=(\n",
    "            json_column.apply(itemgetter(\"explanation_correctness\")).astype(\"category\")\n",
    "        ),\n",
    "        # I wish I had a better name for this column, but it's basically, \"if the explanation is MAYBE correct, WHY is it maybe correct?\"\n",
    "        explanation_maybe=(\n",
    "            json_column.apply(itemgetter(\"explanation_maybe\")).astype(\"category\")\n",
    "        ),\n",
    "        fix=(\n",
    "            json_column.apply(itemgetter(\"fix\")).fillna(\"N/A\").astype(\"category\")\n",
    "        ),\n",
    "        fix_correctness=(\n",
    "            json_column.apply(itemgetter(\"fix_correctness\")).astype(\"category\")\n",
    "        ),\n",
    "        additional_errors=(\n",
    "            json_column.apply(itemgetter(\"additional_errors\")).astype(\"category\")\n",
    "        ),\n",
    "        notes=(\n",
    "            json_column.apply(itemgetter(\"notes\")).astype(\"string\")\n",
    "        ),\n",
    "        length=(\n",
    "            json_column.apply(itemgetter(\"length\")).astype(int)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def variant_as_categorical(df):\n",
    "    variant = df[\"variant\"].astype(\"category\")\n",
    "    assert len(variant.cat.categories) == 4\n",
    "    return df\n",
    "\n",
    "\n",
    "def rater_as_categorical(df):\n",
    "    rater = df[\"rater\"].astype(\"category\")\n",
    "    assert len(rater.cat.categories) == 3\n",
    "    return df.assign(rater=rater)\n",
    "\n",
    "\n",
    "def set_empty_notes_to_na(df):\n",
    "    \"Notes that are empty strings should just be missing values\"\n",
    "    return df.assign(notes=df[\"notes\"].replace(\"\", pd.NA))\n",
    "\n",
    "\n",
    "expanded_df = df\\\n",
    "    .pipe(json_to_columns)\\\n",
    "    .pipe(variant_as_categorical)\\\n",
    "    .pipe(rater_as_categorical)\\\n",
    "    .pipe(set_empty_notes_to_na)\\\n",
    "    .drop(columns=[\"answers\"])\n",
    "\n",
    "expanded_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the levels of the categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the levels for each categorical variable in df\n",
    "def get_levels(df, skip=()):\n",
    "    for column in df.select_dtypes(\"category\").columns:\n",
    "        if column in skip:\n",
    "            continue\n",
    "        print(df[column].value_counts())\n",
    "        print(set(df[column].cat.categories))\n",
    "        print()\n",
    "\n",
    "\n",
    "get_levels(expanded_df, skip=[\"variant\", \"rater\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further data analysis, it's useful to think of some of the categorical responses as being ordinal. It will also smooth over inter-rater reliability (you could argue that it's cooking the books), since if two raters answer \"yes\" and \"maybe\", that's more agreement than two raters saying \"yes\" and \"no\".\n",
    "\n",
    "There are also some columns that can be made binary, like \"does it provide a correct fix?\" or \"is the explanation definitely correct?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_structure_levels = {'unclear': 0, 'could-be-clearer': 1, 'clear': 2}\n",
    "explanation_correctness_levels = {'no': 0, 'maybe': 1, 'yes': 2}\n",
    "fix_levels = {'no': 0, 'implicit-suggestion': 1, 'generic': 2, 'hint': 3, 'confident': 4}\n",
    "fix_correctness_levels = {'no': 0, 'maybe': 1, 'yes': 2}\n",
    "additional_errors_levels = {'no': 0, 'maybe': 1, 'yes': 2}\n",
    "\n",
    "def add_ordinal_columns(df):\n",
    "    return df.assign(\n",
    "        sentence_structure_ordinal=df[\"sentence_structure\"].map(sentence_structure_levels),\n",
    "        explanation_correctness_ordinal=df[\"explanation_correctness\"].map(explanation_correctness_levels),\n",
    "        fix_ordinal=df[\"fix\"].map(fix_levels),\n",
    "        fix_correctness_ordinal=df[\"fix_correctness\"].map(fix_correctness_levels),\n",
    "        additional_errors_ordinal=df[\"additional_errors\"].map(additional_errors_levels),\n",
    "    )\n",
    "\n",
    "def add_binary_columns(df):\n",
    "    return df.assign(\n",
    "        # Is the sentence structure clear?\n",
    "        sentence_structure_binary=df[\"sentence_structure\"].map({\"unclear\": 0, \"could-be-clearer\": 0, \"clear\": 1}),\n",
    "        # Can the explanation be reasonably considered to be correct?\n",
    "        explanation_correctness_binary=df[\"explanation_correctness\"].map({\"no\": 0, \"maybe\": 1, \"yes\": 1}),\n",
    "        # Is **any** kind of fix suggested?\n",
    "        fix_binary=df[\"fix\"].map({'no': 0, 'implicit-suggestion': 1, 'generic': 1, 'hint': 1, 'confident': 1}),\n",
    "        # Is the fix DEFINITELY correct?\n",
    "        fix_correctness_binary=df[\"fix_correctness\"].map({\"no\": 0, \"maybe\": 0, \"yes\": 1}),\n",
    "        additional_errors_binary=df[\"additional_errors\"].map({\"no\": 0, \"maybe\": 1, \"yes\": 1}),\n",
    "    )\n",
    "\n",
    "\n",
    "full_df = expanded_df\\\n",
    "    .pipe(add_ordinal_columns)\\\n",
    "    .pipe(add_binary_columns)\n",
    "\n",
    "full_df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every rating **must** state whether an explanation was provided or not. Assert this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert full_df[\"explanation\"].isna().sum() == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length should be the equal regardless of the rater, so let's assert that here and get a table we can join with later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_lengths(df):\n",
    "    \"Ensure that each rater has seen a message with the same length.\"\n",
    "    assert (df[\"length\"][\"min\"] == df[\"length\"][\"max\"]).all()\n",
    "    return df\n",
    "\n",
    "\n",
    "def as_sql_style_table(df):\n",
    "    return pd.DataFrame({\n",
    "        \"srcml_path\": df[\"srcml_path\"],\n",
    "        \"version\": df[\"version\"],\n",
    "        \"variant\": df[\"variant\"].astype(\"category\"),\n",
    "        \"length\": df[\"length\"][\"min\"],\n",
    "    })\n",
    "\n",
    "# For some bizarre reason, the fact that variant is a categorical\n",
    "# variable completely changes the behavior of groupby.\n",
    "# I have no idea why.\n",
    "# Instead of fighting it, I convert variant to a string column,\n",
    "# perform the groupby, then convert it BACK to a categorical column.\n",
    "message_lengths = full_df\\\n",
    "    .pipe(lambda df: df.assign(variant=df[\"variant\"].astype(\"string\")))\\\n",
    "    .groupby(BY_SCENARIO, as_index=False)\\\n",
    "    .agg({\"length\": [\"min\", \"max\"]})\\\n",
    "    .pipe(assert_lengths)\\\n",
    "    .pipe(as_sql_style_table)\n",
    "\n",
    "message_lengths.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assert that all scenarios were answered by at least two raters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_ratings = full_df.groupby([\"srcml_path\", \"version\"]).filter(lambda x: len(x[\"rater\"].unique()) >= 2)\n",
    "assert len(two_ratings) == len(full_df), \"Had fewer than two ratings for some scenarios\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_COLUMNS = {'srcml_path', 'version', 'variant', 'rater', 'length'}\n",
    "\n",
    "COLUMNS_TO_RENAME = [\n",
    "    column for column in full_df.columns\n",
    "    if column not in FIXED_COLUMNS\n",
    "]\n",
    "\n",
    "# TODO: make a hierarchical index?\n",
    "def get_ratings(df, rater):\n",
    "    return (\n",
    "        df[df[\"rater\"] == rater]\n",
    "        .set_index(BY_SCENARIO)\n",
    "        # We don't need the rater column anymore\n",
    "        # and earlier we confirmed that the length column is identical\n",
    "        .drop(columns=[\"rater\", \"length\"])\n",
    "        # We have to rename the columns here because the three-way merge will fail to recognize identical columns\n",
    "        # otherwise:\n",
    "        .rename(columns={col: f\"{col}_{rater}\" for col in COLUMNS_TO_RENAME})\n",
    "    )\n",
    "\n",
    "# Get the explanation ratings from prajish and eddie:\n",
    "eddie_ratings = get_ratings(two_ratings, \"eddie\")\n",
    "prajish_ratings = get_ratings(two_ratings, \"prajish\")\n",
    "brett_ratings = get_ratings(two_ratings, \"brett\")\n",
    "\n",
    "assert len(eddie_ratings) == len(prajish_ratings) == len(brett_ratings)\n",
    "assert len(eddie_ratings) == len(full_df) / full_df[\"rater\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = prajish_ratings.join([eddie_ratings, brett_ratings], how=\"outer\")\n",
    "\n",
    "ratings.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddie_and_prajish_agreement = ratings[ratings[\"explanation_eddie\"].notna() & ratings[\"explanation_prajish\"].notna()]\n",
    "eddie_and_prajish_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps Scott's pi is a better measure of agreement for this data:\n",
    "# See: https://stats.stackexchange.com/a/525640\n",
    "\n",
    "eddie_and_prajish_agreement = ratings[ratings[\"explanation_eddie\"].notna() & ratings[\"explanation_prajish\"].notna()]\n",
    "\n",
    "kappa = cohen_kappa_score(eddie_and_prajish_agreement[\"explanation_eddie\"].astype(bool), eddie_and_prajish_agreement[\"explanation_prajish\"].astype(bool))\n",
    "kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_two_raters(rater1, rater2):\n",
    "    \"Returns a DataFrame with only the rows where both raters have rated the scenario\"\n",
    "    return ratings[ratings[f\"explanation_{rater1}\"].notna() & ratings[f\"explanation_{rater2}\"].notna()]\n",
    "\n",
    "def kappa_for_two_raters(column, rater1, rater2):\n",
    "    \"Returns agreement for two raters on a certain column\"\n",
    "    agreement = group_two_raters(rater1, rater2)\n",
    "    return cohen_kappa_score(agreement[f\"{column}_{rater1}\"].astype(\"category\"), agreement[f\"{column}_{rater2}\"].astype(\"category\"))\n",
    "\n",
    "def kappa_for_two_raters_with_label(column, rater1, rater2):\n",
    "    kappa = kappa_for_two_raters(column, rater1, rater2)\n",
    "    return kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"eddie\", \"prajish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"eddie\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"prajish\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"eddie\", \"prajish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"eddie\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"prajish\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddie = ratings[\"explanation_eddie\"]\n",
    "brett = ratings[\"explanation_brett\"]\n",
    "prajish = ratings[\"explanation_prajish\"]\n",
    "\n",
    "rater1 = prajish.combine_first(brett)\n",
    "rater2 = eddie.combine_first(brett)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = cohen_kappa_score(rater1.astype(\"category\"), rater2.astype(\"category\"))\n",
    "kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coallesce_agreement(column):\n",
    "    eddie = ratings[f\"{column}_eddie\"]\n",
    "    brett = ratings[f\"{column}_brett\"]\n",
    "    prajish = ratings[f\"{column}_prajish\"]\n",
    "\n",
    "    # For null coallescing to work properly, the alternative option must be the same for both columns\n",
    "    # In other words, Brett must ALWAYS be last:\n",
    "    rater1 = prajish.combine_first(brett).astype(\"category\")\n",
    "    rater2 = eddie.combine_first(brett).astype(\"category\")\n",
    "\n",
    "    assert rater1.isna().sum() == rater2.isna().sum() == 0\n",
    "\n",
    "    kappa = cohen_kappa_score(rater1, rater2)\n",
    "    return kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"explanation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"explanation_correctness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"fix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"fix_correctness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df\\\n",
    "#    .assign(explanation=full_df[\"explanation\"].astype(int))\\\n",
    "#    .pivot(index=[\"srcml_path\", \"version\", \"variant\"], columns=\"rater\", values=[\"explanation\", \"fix\", \"notes\"]).to_excel(\"full_df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-useful-monorepo-alBd81bV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
