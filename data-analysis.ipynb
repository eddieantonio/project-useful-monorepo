{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: to get this working, you must install the `data-analysis` group dependencies using Poetry:\n",
    "\n",
    "```shell\n",
    "poetry install --with=data-analysis\n",
    "```\n",
    "\n",
    "You should also run this if you are using git:\n",
    "\n",
    "```shell\n",
    "poetry run nbstripout --install\n",
    "```\n",
    "\n",
    "This will automatically clear output when committing to git :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from data_analysis_utils import agreement_as_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to index and group data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by each Java source code file (unit):\n",
    "# TODO: rename to \"context\"?\n",
    "BY_UNIT = [\"srcml_path\", \"version\"]\n",
    "# by the PEM variant shown to the rater:\n",
    "BY_SCENARIO = BY_UNIT + [\"variant\"]\n",
    "# by the rater: this uniquely identifies one particular data point:\n",
    "BY_RATER = BY_SCENARIO + [\"rater\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a basic dataframe from `answers.sqlite3`. However, the `answers` column will need to be parsed as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read answers.sqlite3 into a DataFrame\n",
    "conn = sqlite3.connect(\"answers.sqlite3\")\n",
    "df = pd.read_sql_query(\"SELECT * FROM answers\", conn)\n",
    "conn.close()\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to clean the data â€” extract it from that JSON column.\n",
    "\n",
    "Instead of creating a nice schema in `answers.sqlite3`, I decided to defer the job of making nice columns to the data analysis stage. So now we have to parse the answers column as JSON and extract data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As of 2023-05-02, these are columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(df[\"answers\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_columns(df):\n",
    "    json_column = df[\"answers\"].apply(json.loads)\n",
    "    return df.assign(\n",
    "        jargon=(\n",
    "            json_column.apply(itemgetter(\"jargon\")).astype(int)\n",
    "        ),\n",
    "        sentence_structure=(\n",
    "            json_column.apply(itemgetter(\"sentence_structure\")).astype(\"category\")\n",
    "        ),\n",
    "        # Explanation should already be a boolean column, that is completely filled in:\n",
    "        explanation=(\n",
    "            json_column.apply(itemgetter(\"explanation\"))\n",
    "        ),\n",
    "        explanation_correctness=(\n",
    "            json_column.apply(itemgetter(\"explanation_correctness\")).astype(\"category\")\n",
    "        ),\n",
    "        # I wish I had a better name for this column, but it's basically, \"if the explanation is MAYBE correct, WHY is it maybe correct?\"\n",
    "        explanation_maybe=(\n",
    "            json_column.apply(itemgetter(\"explanation_maybe\")).astype(\"category\")\n",
    "        ),\n",
    "        fix=(\n",
    "            json_column.apply(itemgetter(\"fix\")).fillna(\"N/A\").astype(\"category\")\n",
    "        ),\n",
    "        fix_correctness=(\n",
    "            json_column.apply(itemgetter(\"fix_correctness\")).astype(\"category\")\n",
    "        ),\n",
    "        additional_errors=(\n",
    "            json_column.apply(itemgetter(\"additional_errors\")).astype(\"category\")\n",
    "        ),\n",
    "        notes=(\n",
    "            json_column.apply(itemgetter(\"notes\")).astype(\"string\")\n",
    "        ),\n",
    "        length=(\n",
    "            json_column.apply(itemgetter(\"length\")).astype(int)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def variant_as_categorical(df):\n",
    "    variant = df[\"variant\"].astype(\"category\")\n",
    "    assert len(variant.cat.categories) == 4\n",
    "    return df\n",
    "\n",
    "\n",
    "def rater_as_categorical(df):\n",
    "    rater = df[\"rater\"].astype(\"category\")\n",
    "    assert len(rater.cat.categories) == 3\n",
    "    return df.assign(rater=rater)\n",
    "\n",
    "\n",
    "def set_empty_notes_to_na(df):\n",
    "    \"Notes that are empty strings should just be missing values\"\n",
    "    return df.assign(notes=df[\"notes\"].replace(\"\", pd.NA))\n",
    "\n",
    "expanded_df = df\\\n",
    "    .pipe(json_to_columns)\\\n",
    "    .drop(columns=[\"answers\"])\\\n",
    "    .pipe(variant_as_categorical)\\\n",
    "    .pipe(rater_as_categorical)\\\n",
    "    .pipe(set_empty_notes_to_na)\n",
    "\n",
    "expanded_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the levels of the categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the levels for each categorical variable in df\n",
    "def get_levels(df, skip=()):\n",
    "    for column in df.select_dtypes(\"category\").columns:\n",
    "        if column in skip:\n",
    "            continue\n",
    "        print(df[column].value_counts())\n",
    "        print(set(df[column].cat.categories))\n",
    "        print()\n",
    "\n",
    "\n",
    "get_levels(expanded_df, skip=[\"variant\", \"rater\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further data analysis, it's useful to think of some of the categorical responses as being ordinal. It will also smooth over inter-rater reliability (you could argue that it's cooking the books), since if two raters answer \"yes\" and \"maybe\", that's more agreement than two raters saying \"yes\" and \"no\".\n",
    "\n",
    "There are also some columns that can be made binary, like \"does it provide a correct fix?\" or \"is the explanation definitely correct?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMAPPINGS = dict(\n",
    "    sentence_structure_ordinal={\"unclear\": 0, \"could-be-clearer\": 1, \"clear\": 2},\n",
    "    explanation_correctness_ordinal={\"no\": 0, \"maybe\": 1, \"yes\": 2},\n",
    "    fix_ordinal={\n",
    "        \"no\": 0,\n",
    "        \"implicit-suggestion\": 1,\n",
    "        \"generic\": 2,\n",
    "        \"hint\": 3,\n",
    "        \"confident\": 4,\n",
    "    },\n",
    "    fix_correctness_ordinal={\"no\": 0, \"maybe\": 1, \"yes\": 2},\n",
    "    additional_errors_ordinal={\"no\": 0, \"maybe\": 1, \"yes\": 2},\n",
    "    # Is the sentence structure clear?\n",
    "    sentence_structure_binary={\"unclear\": 0, \"could-be-clearer\": 0, \"clear\": 1},\n",
    "    # Can the explanation be reasonably considered to be correct?\n",
    "    explanation_correctness_binary={\"no\": 0, \"maybe\": 1, \"yes\": 1},\n",
    "    # Is **any** kind of fix suggested?\n",
    "    fix_binary_levels={\n",
    "        \"no\": 0,\n",
    "        \"implicit-suggestion\": 1,\n",
    "        \"generic\": 1,\n",
    "        \"hint\": 1,\n",
    "        \"confident\": 1,\n",
    "    },\n",
    "    # Is the fix DEFINITELY correct?\n",
    "    fix_correctness_binary={\"no\": 0, \"maybe\": 0, \"yes\": 1},\n",
    "    additional_errors_binary={\"no\": 0, \"maybe\": 1, \"yes\": 1},\n",
    ")\n",
    "\n",
    "def with_correct_dtype(df, column_name):\n",
    "    return df[column_name].map(REMAPPINGS[column_name])\n",
    "\n",
    "\n",
    "def add_ordinal_columns(df):\n",
    "    return df.assign(\n",
    "        sentence_structure_ordinal=df[\"sentence_structure\"].map(REMAPPINGS[\"sentence_structure_ordinal\"]),\n",
    "        explanation_correctness_ordinal=df[\"explanation_correctness\"].map(REMAPPINGS[\"explanation_correctness_ordinal\"]),\n",
    "        fix_ordinal=df[\"fix\"].map(REMAPPINGS[\"fix_ordinal\"]),\n",
    "        fix_correctness_ordinal=df[\"fix_correctness\"].map(REMAPPINGS[\"fix_correctness_ordinal\"]),\n",
    "        additional_errors_ordinal=df[\"additional_errors\"].map(REMAPPINGS[\"additional_errors_ordinal\"]),\n",
    "    )\n",
    "\n",
    "\n",
    "def add_binary_columns(df):\n",
    "    return df.assign(\n",
    "        sentence_structure_binary=df[\"sentence_structure\"].map(REMAPPINGS[\"sentence_structure_binary\"]),\n",
    "        explanation_correctness_binary=df[\"explanation_correctness\"].map(REMAPPINGS[\"explanation_correctness_binary\"]),\n",
    "        fix_binary=df[\"fix\"].map(REMAPPINGS[\"fix_binary_levels\"]),\n",
    "        fix_correctness_binary=df[\"fix_correctness\"].map(REMAPPINGS[\"fix_correctness_binary\"]),\n",
    "        additional_errors_binary=df[\"additional_errors\"].map(REMAPPINGS[\"additional_errors_binary\"]),\n",
    "    )\n",
    "\n",
    "\n",
    "full_df = expanded_df\\\n",
    "    .pipe(add_ordinal_columns)\\\n",
    "    .pipe(add_binary_columns)\n",
    "\n",
    "full_df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every rating **must** state whether an explanation was provided or not. Assert this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert full_df[\"explanation\"].isna().sum() == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length should be the equal regardless of the rater, so let's assert that here and get a table we can join with later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_lengths(df):\n",
    "    \"Ensure that each rater has seen a message with the same length.\"\n",
    "    assert (df[\"length\"][\"min\"] == df[\"length\"][\"max\"]).all()\n",
    "    return df\n",
    "\n",
    "\n",
    "def as_sql_style_table(df):\n",
    "    return pd.DataFrame({\n",
    "        \"srcml_path\": df[\"srcml_path\"],\n",
    "        \"version\": df[\"version\"],\n",
    "        \"variant\": df[\"variant\"],\n",
    "        \"length\": df[\"length\"][\"min\"],\n",
    "    })\n",
    "\n",
    "\n",
    "# A common pitfall of using a categorical variable with groupby\n",
    "# is that it does \"some sort of cartesian product\" with the other columns,\n",
    "# which is not what we want.\n",
    "# So we have to set `observed=False` to avoid this.\n",
    "# \n",
    "# See: https://stackoverflow.com/a/67645084\n",
    "message_lengths = full_df\\\n",
    "    .groupby(BY_SCENARIO, as_index=False, observed=False)\\\n",
    "    .agg({\"length\": [\"min\", \"max\"]})\\\n",
    "    .pipe(assert_lengths)\\\n",
    "    .pipe(as_sql_style_table)\n",
    "\n",
    "message_lengths.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assert that all scenarios were answered by at least two raters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_ratings = full_df.groupby([\"srcml_path\", \"version\"]).filter(lambda x: len(x[\"rater\"].unique()) >= 2)\n",
    "assert len(two_ratings) == len(full_df), \"Had fewer than two ratings for some scenarios\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the ratings of all of raters into one, big table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_COLUMNS = {'srcml_path', 'version', 'variant', 'rater', 'length'}\n",
    "\n",
    "RESPONSE_COLUMNS = [\n",
    "    column for column in full_df.columns\n",
    "    if column not in FIXED_COLUMNS\n",
    "]\n",
    "RESPONSE_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_assigned(df):\n",
    "    \"\"\"\n",
    "    Add convenience columns that indicates whether a rater was assigned to this scenario.\n",
    "    This only makes sense AFTER pivoting the DataFrame.\n",
    "    \"\"\"\n",
    "    # A rater has given True or False if and only if they were assigned to this scenario.\n",
    "    # Therefore, we can use the presence of NaNs to determine whether a rater was assigned:\n",
    "    assigned = df[\"explanation\"].notna()\n",
    "    # However... we need to reconstruct the hierarchical index before we concatentate it.\n",
    "    index = pd.MultiIndex.from_product([[\"assigned\"], assigned.columns])\n",
    "    assigned_df = pd.DataFrame(assigned.values, columns=index, index=assigned.index)\n",
    "    # I tried to do this using DataFrame.assign, but this doesn't work for hierarchical indexes.\n",
    "    return pd.concat([df, assigned_df], axis=1)\n",
    "\n",
    "# Put all the raters side-by-side using a hierarchical index\n",
    "ratings = full_df\\\n",
    "    .drop(columns=[\"length\"])\\\n",
    "    .pivot(index=BY_SCENARIO, columns=\"rater\", values=RESPONSE_COLUMNS)\\\n",
    "    .pipe(add_assigned)\n",
    "\n",
    "ratings.head(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's nice to see this data in an Excel spreadsheet, so OPTIONALLY export it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals().get(\"EXPORT_EXCEL\", False):\n",
    "    ratings.to_excel(\"hierarchical_responses.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: perhaps Scott's Ï€ is a better measure of agreement for this data:\n",
    "\n",
    "See: https://stats.stackexchange.com/a/525640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_two_raters(rater1, rater2):\n",
    "    \"Returns a DataFrame with only the rows where both raters have rated the scenario\"\n",
    "    return ratings[ratings[\"assigned\"][rater1] & ratings[\"assigned\"][rater2]]\n",
    "\n",
    "def kappa_for_two_raters(column, rater1, rater2):\n",
    "    \"Returns agreement for two raters on a certain column\"\n",
    "    agreement = group_two_raters(rater1, rater2)\n",
    "    criteria = agreement[column].astype(\"category\")\n",
    "    return cohen_kappa_score(criteria[rater1], criteria[rater2])\n",
    "\n",
    "def kappa_for_two_raters_with_label(column, rater1, rater2):\n",
    "    kappa = kappa_for_two_raters(column, rater1, rater2)\n",
    "    return kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"eddie\", \"prajish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"eddie\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"prajish\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"eddie\", \"prajish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"eddie\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"prajish\", \"brett\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesise two raters by null-coalescing our three raters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_coalesce_rater1(row):\n",
    "    # prefer Eddie over Brett\n",
    "    return row[\"brett\"] if pd.isna(row[\"eddie\"]) else row[\"eddie\"]\n",
    "\n",
    "def null_coalesce_rater2(row):\n",
    "    # prefer Prajish over Brett\n",
    "    return row[\"brett\"] if pd.isna(row[\"prajish\"]) else row[\"prajish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coallesce_agreement(column, type_=\"category\", **kwargs):\n",
    "    criteria = ratings[column]\n",
    "\n",
    "    # We need to correct the dtypes, because pivoting the table\n",
    "    # introduced missing values, and reverted the data types to object.\n",
    "    rater1 = to_correct_dtype(criteria.apply(null_coalesce_rater1, axis=1), column)\n",
    "    rater2 = to_correct_dtype(criteria.apply(null_coalesce_rater2, axis=1), column)\n",
    "\n",
    "    assert rater1.isnull().sum() == 0\n",
    "    assert rater2.isnull().sum() == 0\n",
    "\n",
    "    kappa = cohen_kappa_score(rater1, rater2, **kwargs)\n",
    "    return kappa, agreement_as_label(kappa)\n",
    "\n",
    "\n",
    "def to_correct_dtype(column, column_name):\n",
    "    type_ = column_name.split(\"_\")[-1]\n",
    "    assert column.isnull().sum() == 0, f\"Column {column_name} has missing values\"\n",
    "    if type_ == \"binary\":\n",
    "        return column.astype(bool)\n",
    "    elif column_name == \"jargon\" or type_ == \"ordinal\":\n",
    "        return column.astype(int)\n",
    "    else:\n",
    "        return column.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_NULL_COLUMNS = [\n",
    "    \"jargon\",\n",
    "    \"sentence_structure\",\n",
    "    \"explanation\",\n",
    "    \"fix\",\n",
    "    \"additional_errors\",\n",
    "    \"notes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"jargon\", weights=\"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"explanation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"fix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"additional_errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"fix_binary\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ordinal data, apply weighting to smooth the agreement.\n",
    "\n",
    "See here for weighting: https://github.com/jmgirard/mReliability/wiki/Weighting-scheme\n",
    "\n",
    "I think linear weighting is fair. Quadratic is waaaaay too forgiving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"sentence_structure_ordinal\", weights=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"fix_ordinal\", weights=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"additional_errors_ordinal\", weights=\"linear\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: How to measure the agreement of columns with nulls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_for_two_raters(df, column_name):\n",
    "    criteria = df[column_name]\n",
    "    #rater1 = to_correct_dtype(criteria.apply(null_coalesce_rater1, axis=1), column_name)\n",
    "    #rater2 = to_correct_dtype(criteria.apply(null_coalesce_rater2, axis=1), column_name)\n",
    "    rater2 = criteria.apply(null_coalesce_rater2, axis=1)\n",
    "    rater1 = criteria.apply(null_coalesce_rater1, axis=1)\n",
    "    index = pd.MultiIndex.from_product([[column_name], [\"rater1\", \"rater2\"]])\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            (column_name, \"rater1\"): rater1,\n",
    "            (column_name, \"rater2\"): rater2,\n",
    "        },\n",
    "        columns=index, index=df.index\n",
    "    )\n",
    "\n",
    "two_raters = pd.concat([column_for_two_raters(ratings, column) for column in RESPONSE_COLUMNS], axis=1)\n",
    "if globals().get(\"EXPORT_EXCEL\", False):\n",
    "    two_raters.to_excel(\"two_raters.xlsx\")\n",
    "two_raters.head(8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-useful-monorepo-alBd81bV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
