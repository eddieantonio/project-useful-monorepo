{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: to get this working, you must install the `data-analysis` group dependencies using Poetry:\n",
    "\n",
    "```shell\n",
    "poetry install --with=data-analysis\n",
    "```\n",
    "\n",
    "You should also run this if you are using git:\n",
    "\n",
    "```shell\n",
    "poetry run nbstripout --install\n",
    "```\n",
    "\n",
    "This will automatically clear output when committing to git :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a basic dataframe from `answers.sqlite3`. Then we'll tease out the JSON columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from data_analysis_utils import agreement_as_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to index and group data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by each Java source code file (unit):\n",
    "BY_UNIT = [\"srcml_path\", \"version\"]\n",
    "# by the PEM variant shown to the rater:\n",
    "BY_SCENARIO = BY_UNIT + [\"variant\"]\n",
    "# by the rater: uniquely identifies one particular data point:\n",
    "BY_RATER = BY_SCENARIO + [\"rater\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read answers.sqlite3 into a DataFrame\n",
    "conn = sqlite3.connect(\"answers.sqlite3\")\n",
    "df = pd.read_sql_query(\"SELECT * FROM answers\", conn)\n",
    "conn.close()\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to clean the data â€” extract it from that JSON column.\n",
    "\n",
    "Instead of creating a nice schema in `answers.sqlite3`, I decided to defer the job of making nice columns to the data analysis stage. So now we have to parse the answers column as JSON and extract data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As of 2023-05-02, these are columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"answers\"].apply(json.loads)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "df[\"answers\"].apply(json.loads).apply(itemgetter(\"fix\")).value_counts()\n",
    "df[\"answers\"].apply(json.loads).apply(lambda x: x[\"fix\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def json_to_columns(df):\n",
    "    json_column = df[\"answers\"].apply(json.loads)\n",
    "    return df.assign(\n",
    "        jargon=(\n",
    "            json_column.apply(itemgetter(\"jargon\")).astype(int)\n",
    "        ),\n",
    "        sentence_structure=(\n",
    "            json_column.apply(itemgetter(\"sentence_structure\")).astype(\"category\")\n",
    "        ),\n",
    "        # Explanation should already be a boolean column, that is completely filled in:\n",
    "        explanation=(\n",
    "            json_column.apply(itemgetter(\"explanation\"))\n",
    "        ),\n",
    "        explanation_correctness=(\n",
    "            json_column.apply(itemgetter(\"explanation_correctness\")).astype(\"category\")\n",
    "        ),\n",
    "        # I wish I had a better name for this column, but it's basically, \"if the explanation is MAYBE correct, WHY is it maybe correct?\"\n",
    "        explanation_maybe=(\n",
    "            json_column.apply(itemgetter(\"explanation_maybe\")).astype(\"category\")\n",
    "        ),\n",
    "        fix=(\n",
    "            json_column.apply(itemgetter(\"fix\")).fillna(\"N/A\").astype(\"category\")\n",
    "        ),\n",
    "        fix_correctness=(\n",
    "            json_column.apply(itemgetter(\"fix_correctness\")).astype(\"category\")\n",
    "        ),\n",
    "        additional_errors=(\n",
    "            json_column.apply(itemgetter(\"additional_errors\")).astype(\"category\")\n",
    "        ),\n",
    "        notes=(\n",
    "            json_column.apply(itemgetter(\"notes\")).astype(\"string\")\n",
    "        ),\n",
    "        length=(\n",
    "            json_column.apply(itemgetter(\"length\")).astype(int)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def variant_and_rater_as_categorical(df):\n",
    "    \"variant and rater are both fixed, categorical variables\"\n",
    "    return df.assign(\n",
    "        variant=df[\"variant\"].astype(\"category\"),\n",
    "        rater=df[\"rater\"].astype(\"category\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def variant_as_categorical(df):\n",
    "    assert df[\"variant\"].nunique() == 4\n",
    "    # For some bizarre reason, even though variant IS categorical data,\n",
    "    # when Panadas uses it in a groupby, it messes things up.\n",
    "    # Specifically, it will make facets for each variant.\n",
    "    # I don't understand why. So keep it as is.\n",
    "    return df\n",
    "\n",
    "\n",
    "def rater_as_categorical(df):\n",
    "    rater = df[\"rater\"].astype(\"category\")\n",
    "    assert len(rater.cat.categories) == 3\n",
    "    return df.assign(rater=rater)\n",
    "\n",
    "\n",
    "def set_empty_notes_to_na(df):\n",
    "    \"Notes that are empty strings should just be missing values\"\n",
    "    return df.assign(notes=df[\"notes\"].replace(\"\", pd.NA))\n",
    "\n",
    "\n",
    "full_df = df.pipe(json_to_columns)\\\n",
    "    .pipe(variant_as_categorical)\\\n",
    "    .pipe(rater_as_categorical)\\\n",
    "    .pipe(set_empty_notes_to_na)\\\n",
    "    .drop(columns=[\"answers\"])\n",
    "\n",
    "full_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert full_df[\"explanation\"].isna().sum() == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length should be the equal regardless of the rater, so let's assert that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lengths(df):\n",
    "    lengths = df.groupby(BY_SCENARIO).agg({\"length\": [\"min\", \"max\"]})\n",
    "    assert (lengths[\"length\"][\"min\"] == lengths[\"length\"][\"max\"]).all()\n",
    "\n",
    "# Note: requires variant to be an object column... for some reason\n",
    "check_lengths(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.groupby(BY_SCENARIO).agg({\"length\": [\"min\", \"max\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df\\\n",
    "    .pipe(lambda df: df.assign(variant=df[\"variant\"].astype(\"category\")))\\\n",
    "    .groupby(BY_SCENARIO)\\\n",
    "    .agg({\"length\": [\"min\", \"max\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    \"javac\": 0,\n",
    "    \"decaf\": 1,\n",
    "    \"gpt-4-error-only\": 2,\n",
    "    \"gpt-4-with-context\": 3,\n",
    "}\n",
    "\n",
    "full_df\\\n",
    "    .pipe(lambda df: df.assign(variant=df[\"variant\"].replace(mapper)))\\\n",
    "    .groupby(BY_SCENARIO)\\\n",
    "    .agg({\"length\": [\"min\", \"max\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    \"javac\": 0,\n",
    "    \"decaf\": 1,\n",
    "    \"gpt-4-error-only\": 2,\n",
    "    \"gpt-4-with-context\": 3,\n",
    "}\n",
    "\n",
    "def assert_lengths(df):\n",
    "    \"Ensure that each rater has seen a message with the same length.\"\n",
    "    assert (df[\"length\"][\"min\"] == df[\"length\"][\"max\"]).all()\n",
    "    return df\n",
    "\n",
    "full_df\\\n",
    "    .pipe(lambda df: df.assign(variant=df[\"variant\"].astype(\"category\")))\\\n",
    "    .groupby(BY_SCENARIO, as_index=False)\\\n",
    "    .size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    \"javac\": 0,\n",
    "    \"decaf\": 1,\n",
    "    \"gpt-4-error-only\": 2,\n",
    "    \"gpt-4-with-context\": 3,\n",
    "}\n",
    "\n",
    "def assert_lengths(df):\n",
    "    \"Ensure that each rater has seen a message with the same length.\"\n",
    "    assert (df[\"length\"][\"min\"] == df[\"length\"][\"max\"]).all()\n",
    "    return df\n",
    "\n",
    "def to_length_column(df):\n",
    "    return df.assign(length=pd.Series(df[\"length\"][\"min\"]))\n",
    "\n",
    "def as_sql_style_table(df):\n",
    "    return pd.DataFrame({\n",
    "        \"srcml_path\": df[\"srcml_path\"],\n",
    "        \"version\": df[\"version\"],\n",
    "        \"variant\": df[\"variant\"].astype(\"category\"),\n",
    "        \"length\": df[\"length\"][\"min\"],\n",
    "    })\n",
    "\n",
    "full_df\\\n",
    "    .pipe(lambda df: df.assign(variant=df[\"variant\"].astype(\"string\")))\\\n",
    "    .groupby(BY_SCENARIO, as_index=False)\\\n",
    "    .agg({\"length\": [\"min\", \"max\"]})\\\n",
    "    .pipe(assert_lengths)\\\n",
    "    .pipe(as_sql_style_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_ratings = full_df.groupby([\"srcml_path\", \"version\"]).filter(lambda x: len(x[\"rater\"].unique()) >= 2)\n",
    "assert len(two_ratings) == len(full_df), \"Had fewer than two ratings for some scenarios\"\n",
    "\n",
    "two_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_RENAME = [\n",
    "    \"jargon\",\n",
    "    \"sentence_structure\",\n",
    "    \"explanation\",\n",
    "    \"explanation_correctness\",\n",
    "    \"explanation_maybe\",\n",
    "    \"fix\",\n",
    "    \"fix_correctness\",\n",
    "    \"additional_errors\",\n",
    "    \"notes\",\n",
    "    \"length\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_ratings(df, rater):\n",
    "    return (\n",
    "        df[df[\"rater\"] == rater]\n",
    "        .set_index(BY_SCENARIO)\n",
    "        # We don't need the rater column anymore\n",
    "        # and earlier we confirmed that the length column is identical\n",
    "        .drop(columns=[\"rater\", \"length\"])\n",
    "        # We have to rename the columns here because the three-way merge will fail to recognize identical columns\n",
    "        # otherwise:\n",
    "        .rename(columns={col: f\"{col}_{rater}\" for col in COLUMNS_TO_RENAME})\n",
    "    )\n",
    "\n",
    "# Get the explanation ratings from prajish and eddie:\n",
    "eddie_ratings = get_ratings(two_ratings, \"eddie\")\n",
    "prajish_ratings = get_ratings(two_ratings, \"prajish\")\n",
    "brett_ratings = get_ratings(two_ratings, \"brett\")\n",
    "\n",
    "assert len(eddie_ratings) == len(prajish_ratings) == len(brett_ratings)\n",
    "assert len(eddie_ratings) == 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = prajish_ratings.join([eddie_ratings, brett_ratings], how=\"outer\")\n",
    "\n",
    "ratings.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddie_and_prajish_agreement = ratings[ratings[\"explanation_eddie\"].notna() & ratings[\"explanation_prajish\"].notna()]\n",
    "eddie_and_prajish_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps Scott's pi is a better measure of agreement for this data:\n",
    "# See: https://stats.stackexchange.com/a/525640\n",
    "\n",
    "eddie_and_prajish_agreement = ratings[ratings[\"explanation_eddie\"].notna() & ratings[\"explanation_prajish\"].notna()]\n",
    "\n",
    "kappa = cohen_kappa_score(eddie_and_prajish_agreement[\"explanation_eddie\"].astype(bool), eddie_and_prajish_agreement[\"explanation_prajish\"].astype(bool))\n",
    "kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_two_raters(rater1, rater2):\n",
    "    \"Returns a DataFrame with only the rows where both raters have rated the scenario\"\n",
    "    return ratings[ratings[f\"explanation_{rater1}\"].notna() & ratings[f\"explanation_{rater2}\"].notna()]\n",
    "\n",
    "def kappa_for_two_raters(column, rater1, rater2):\n",
    "    \"Returns agreement for two raters on a certain column\"\n",
    "    agreement = group_two_raters(rater1, rater2)\n",
    "    return cohen_kappa_score(agreement[f\"{column}_{rater1}\"].astype(\"category\"), agreement[f\"{column}_{rater2}\"].astype(\"category\"))\n",
    "\n",
    "def kappa_for_two_raters_with_label(column, rater1, rater2):\n",
    "    kappa = kappa_for_two_raters(column, rater1, rater2)\n",
    "    return kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"eddie\", \"prajish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"eddie\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"explanation\", \"prajish\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"eddie\", \"prajish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"eddie\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_for_two_raters_with_label(\"fix\", \"prajish\", \"brett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddie = ratings[\"explanation_eddie\"]\n",
    "brett = ratings[\"explanation_brett\"]\n",
    "prajish = ratings[\"explanation_prajish\"]\n",
    "\n",
    "rater1 = prajish.combine_first(brett)\n",
    "rater2 = eddie.combine_first(brett)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = cohen_kappa_score(rater1.astype(\"category\"), rater2.astype(\"category\"))\n",
    "kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coallesce_agreement(column):\n",
    "    eddie = ratings[f\"{column}_eddie\"]\n",
    "    brett = ratings[f\"{column}_brett\"]\n",
    "    prajish = ratings[f\"{column}_prajish\"]\n",
    "\n",
    "    # For null coallescing to work properly, the alternative option must be the same for both columns\n",
    "    # In other words, Brett must ALWAYS be last:\n",
    "    rater1 = prajish.combine_first(brett).astype(\"category\")\n",
    "    rater2 = eddie.combine_first(brett).astype(\"category\")\n",
    "\n",
    "    assert rater1.isna().sum() == rater2.isna().sum() == 0\n",
    "\n",
    "    kappa = cohen_kappa_score(rater1, rater2)\n",
    "    return kappa, agreement_as_label(kappa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a summary that is (somewhat) easy to display in Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"explanation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"explanation_correctness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"fix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coallesce_agreement(\"fix_correctness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df\\\n",
    "#    .assign(explanation=full_df[\"explanation\"].astype(int))\\\n",
    "#    .pivot(index=[\"srcml_path\", \"version\", \"variant\"], columns=\"rater\", values=[\"explanation\", \"fix\", \"notes\"]).to_excel(\"full_df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-useful-monorepo-alBd81bV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
